# 神经机器翻译项目实现计划

## 1. 项目结构搭建
- 创建项目基本目录结构
- 配置环境依赖（PyTorch、Transformers等）
- 编写数据加载和预处理模块

## 2. 数据预处理实现
- **数据加载与合并**：读取并合并训练集文件
- **文本清洗**：去除特殊字符、标准化空格和标点
- **词汇表构建**：为中英文分别构建词汇表，实现适当的截断策略
- **序列处理**：实现文本向量化、序列截断与填充
- **数据划分验证**：确保训练/验证/测试集严格分离

## 3. 基于RNN的NMT系统实现
- **基础模型架构**：使用GRU或LSTM构建两层单向seq2seq模型
- **注意力机制**：
  - 实现点积注意力（Dot-product attention）
  - 实现乘法注意力（Multiplicative attention）
  - 实现加性注意力（Additive attention）
- **训练与评估**：
  - 实现训练循环和损失计算
  - 实现BLEU分数评估
  - 添加早停机制和模型保存

## 4. 基于Transformer的NMT系统实现
- **基础架构**：从零构建编码器-解码器结构的Transformer
- **位置嵌入方案**：
  - 实现绝对位置嵌入（Absolute positional encoding）
  - 实现相对位置嵌入（Relative positional encoding）
- **归一化方法**：
  - 实现LayerNorm
  - 实现RMSNorm
- **训练与评估**：同样实现训练循环、BLEU评估和模型保存

## 5. 实验与比较
- 针对不同注意力机制的RNN模型进行训练和评估
- 针对不同位置嵌入和归一化方法的Transformer模型进行训练和评估
- 比较RNN与Transformer架构在翻译质量、训练速度、推理速度和计算复杂度方面的差异
- 生成实验报告和可视化结果

## 6. 文件结构设计
```
├── data/              # 数据相关代码
│   ├── __init__.py
│   ├── preprocessor.py    # 数据预处理
│   ├── dataset.py         # 数据集类
│   └── vocabulary.py      # 词汇表类
├── models/            # 模型定义
│   ├── __init__.py
│   ├── rnn/           # RNN相关模型
│   │   ├── __init__.py
│   │   ├── encoder.py
│   │   ├── decoder.py
│   │   └── attention.py
│   └── transformer/   # Transformer相关模型
│       ├── __init__.py
│       ├── encoder.py
│       ├── decoder.py
│       ├── attention.py
│       └── embedding.py
├── train.py           # 训练脚本
├── evaluate.py        # 评估脚本
├── translate.py       # 翻译推理脚本
├── config.py          # 配置文件
└── requirements.txt   # 依赖库
```