## 1. 中文分词从jieba迁移到HanLP

### 实现步骤：

* **步骤1**：修改导入语句，将`import jieba`替换为HanLP相关导入

* **步骤2**：更新`__init__`方法，初始化HanLP分词器

* **步骤3**：修改`tokenize`方法，将中文分词从`jieba.lcut(text)`替换为HanLP分词

* **步骤4**：确保HanLP分词结果转换为与原实现一致的格式（空格分隔的字符串）

### 技术要点：

* 使用HanLP的`HanLP.segment()`或`StandardTokenizer`进行中文分词

* 将HanLP返回的分词结果转换为字符串列表

* 保持原有的文本预处理逻辑不变

## 2. 英语文本添加小写转换

### 实现步骤：

* **步骤1**：修改`clean_text`方法

* **步骤2**：在所有其他文本清洗步骤之前，添加英文文本小写转换

* **步骤3**：确保该转换只对英文文本生效，不影响中文文本处理

### 技术要点：

* 在`clean_text`方法开始处添加：`if language == 'en': text = text.lower()`

* 确保转换在移除控制字符、多余空格等步骤之前执行

* 不修改其他已有的文本处理逻辑

<br />

